import re

def tokenize(text: str) -> list[str]:
    t = r'[\w]+(?:-[\w]+)*'
    l = re.findall(t, text, re.UNICODE)
    return l

a = "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"
b = "hello,world!!!"
c = "–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"
d = "2025 –≥–æ–¥"
e = "emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"
print(tokenize(a))
print(tokenize(b))
print(tokenize(c))
print(tokenize(d))
print(tokenize(e))